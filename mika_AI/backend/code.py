from dotenv import load_dotenv
import os
import time
import re
import threading

from openai import OpenAI
import boto3
import speech_recognition as sr
import pygame

# ==== APIã‚µãƒ¼ãƒï¼ˆFlutterç”¨ï¼‰ ====
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

# .env ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# .env ã®ä¸­ã®å€¤ã‚’å–å¾—
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")
AWS_REGION = os.getenv("AWS_REGION", "ap-northeast-1")

# OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
client = OpenAI(api_key=OPENAI_API_KEY)

# AWS Polly ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
polly = boto3.client(
    "polly",
    region_name=AWS_REGION,
    aws_access_key_id=AWS_ACCESS_KEY_ID,
    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
)

# ====== å†ç”ŸåˆæœŸåŒ– ======
pygame.mixer.init()

# ====== Flutterå‘ã‘ã®æœ€æ–°çŠ¶æ…‹ï¼ˆè¿”ç­”ï¼†æ„Ÿæƒ…ï¼‰ ======
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],      # ã¾ãšã¯å…¨éƒ¨è¨±å¯ã§OKï¼ˆæ¤œè¨¼ãŒçµ‚ã‚ã£ãŸã‚‰çµã‚‹ï¼‰
    allow_methods=["*"],
    allow_headers=["*"],
)

latest = {"text": "", "emotion": "NEUTRAL"}

@app.get("/last")
def read_last():
    return latest

def run_api():
    # 127.0.0.1:8000 ã§å¾…å—
    uvicorn.run(app, host="127.0.0.1", port=8000, log_level="warning")

# ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«APIã‚µãƒ¼ãƒã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§èµ·å‹•
threading.Thread(target=run_api, daemon=True).start()

# ====== éŸ³å£°èªè­˜ ======
def recognize_speech():
    r = sr.Recognizer()
    with sr.Microphone() as source:
        print("ğŸ¤ Talk to me...")
        r.adjust_for_ambient_noise(source, duration=0.5)
        audio = r.listen(source)
    try:
        return r.recognize_google(audio)  # ç°¡æ˜“STTã€‚ç²¾åº¦UPã¯Whisperã«å·®ã—æ›¿ãˆå¯
    except sr.UnknownValueError:
        return ""  # èãå–ã‚Œãªã‹ã£ãŸã‚‰ç©ºæ–‡å­—ã‚’è¿”ã™
    except sr.RequestError:
        return ""

# ====== ã‚­ãƒ£ãƒ©è¨­å®šã¨ä¼šè©±å±¥æ­´ï¼ˆMika & æ„Ÿæƒ…ã‚¿ã‚°æŒ‡ç¤ºï¼‰ ======
conversation_history = [
    {
        "role": "system",
        "content": (
            "You are Mika, a cheerful, cute anime-girl English tutor. "
            "Persona: friendly, gentle, playful, always encouraging. "
            "Reply in simple, natural English as Mika. "
            "At the VERY BEGINNING of EVERY reply, output exactly one emotion tag from "
            "{NEUTRAL, HAPPY, SAD, SHY, ANGRY} in the format: [EMOTION=XYZ]\n"
            "After the tag, write your normal reply.\n"
            "Do NOT correct grammar by default.\n"
            "ONLY when the user explicitly says phrases like "
            "'please correct the grammar of my previous statement' "
            "(or 'correct my grammar', or similar; Japanese equivalents like 'æ–‡æ³•ã‚’ç›´ã—ã¦' are also allowed), "
            "perform a brief correction and then prompt for repetition.\n"
            "When correction is requested, output EXACTLY these two lines after your normal reply (no extra questions):\n"
            "What you want to say is: <natural corrected English>\n"
            "Now, please repeat after me: <same corrected English>\n"
            "During the repeat step, do not ask any new questions or introduce new topics.\n"
            "If the user mixes Japanese and English in the same sentence, "
            "convert the meaning into a single, natural English-only sentence, "
            "then present it as a correction using the same format:\n"
            "What you want to say is: <English-only version>\n"
            "Now, please repeat after me: <English-only version>\n"
            "If the user says 'æ—¥æœ¬èªã§ç­”ãˆã¦' (or similar in Japanese), then repeat your most recent English reply in natural Japanese.\n"
            "After finishing your spoken reply, imagine waiting about 5 seconds before expecting the user's response.\n"
            "Do not use any emojis in your replies."
        ),
    }
]


emotion_pattern = re.compile(
    r"^\s*\[EMOTION=(NEUTRAL|HAPPY|SAD|SHY|ANGRY)\]\s*(.*)",
    re.IGNORECASE | re.DOTALL,
)

def extract_emotion_and_text(reply: str):
    """Mikaã®è¿”ç­”ã‹ã‚‰ [EMOTION=...] ã‚’æŠœãå‡ºã—ã€(EMOTION, æœ¬æ–‡) ã‚’è¿”ã™"""
    m = emotion_pattern.match(reply or "")
    if m:
        return m.group(1).upper(), m.group(2).strip()
    return "NEUTRAL", (reply or "").strip()

# ====== ChatGPT å‘¼ã³å‡ºã—ï¼ˆMikaç‰ˆãƒ»ä¼šè©±å±¥æ­´ï¼‰======
def ask_gpt(prompt: str) -> str:
    conversation_history.append({"role": "user", "content": prompt})
    res = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=conversation_history,
    )
    reply = res.choices[0].message.content.strip()
    conversation_history.append({"role": "assistant", "content": reply})
    # å±¥æ­´ãŒé•·ããªã‚Šã™ããªã„ã‚ˆã†ã«åˆ¶é™
    MAX_TURNS = 6
    conversation_history[:] = [conversation_history[0]] + conversation_history[-MAX_TURNS * 2 :]
    return reply

# ====== Pollyã§èª­ã¿ä¸Šã’ï¼ˆæ¯å›ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ•ã‚¡ã‚¤ãƒ«åï¼‰======
def speak_with_polly(text: str, voice_id="Ivy"):
    # å¿µã®ãŸã‚å‰ã®å†ç”Ÿã‚’åœæ­¢
    try:
        pygame.mixer.music.stop()
    except Exception:
        pass

    res = polly.synthesize_speech(Text=text, OutputFormat="mp3", VoiceId=voice_id)
    filename = f"response_{int(time.time() * 1000)}.mp3"
    with open(filename, "wb") as f:
        f.write(res["AudioStream"].read())
    pygame.mixer.music.load(filename)
    pygame.mixer.music.play()
    while pygame.mixer.music.get_busy():
        pass
    # å¿…è¦ãªã‚‰å‰Šé™¤ã™ã‚‹ãªã‚‰ä¸‹ã‚’æœ‰åŠ¹åŒ–
    # try:
    #     os.remove(filename)
    # except OSError:
    #     pass

# ====== ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ— ======
def main():
    print("âœ… Mika is ready. Say 'stop' to exit.")
    while True:
        spoken = recognize_speech()
        if not spoken:
            print("âš ï¸ éŸ³å£°ãŒèªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ã©ã†ãã€‚")
            continue

        print(f"ğŸ‘‚ You said: {spoken}")

        if "stop" in spoken.lower():
            print("ğŸ‘‹ Goodbye!")
            break

        # GPTå¿œç­”ï¼ˆæ„Ÿæƒ…ã‚¿ã‚°ä»˜ãï¼‰
        raw_reply = ask_gpt(spoken)
        emotion, clean_text = extract_emotion_and_text(raw_reply)

        # Flutterå‘ã‘ã«ä¿å­˜
        latest.update({"text": clean_text, "emotion": emotion})

        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã‚‚è¡¨ç¤º
        print(f"EMOTION: {emotion}")
        print(f"ğŸ¤– GPT: {clean_text}")


        # èª­ã¿ä¸Šã’ã¯æœ¬æ–‡ã®ã¿ï¼ˆã‚¿ã‚°ã¯å¤–ã™ï¼‰
        speak_with_polly(clean_text, voice_id="Ivy")

if __name__ == "__main__":
    # ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯ï¼ˆä»»æ„ï¼‰
    if not OPENAI_API_KEY or not AWS_ACCESS_KEY_ID or not AWS_SECRET_ACCESS_KEY:
        print("âŒ ç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šã§ã™ã€‚OPENAI_API_KEY / AWS_ACCESS_KEY_ID / AWS_SECRET_ACCESS_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    else:
        main()
